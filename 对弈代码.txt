import numpy as np
import tkinter as tk
from tkinter import messagebox
import torch
from typing import List, Tuple, Union
import torch.nn as nn
import torch.nn.functional as F
import math
import copy
import random

# ================================
# 游戏类及其逻辑
# ================================

# 棋盘类
class ChessBoard
    def __init__(self, board_len int = 15, n_feature_planes int = 17, search_radius int = 2)
        self.board_len = board_len
        self.current_player = 1  # 1 for black, -1 for white
        self.state = {}
        self.n_feature_planes = n_feature_planes
        self.search_radius = search_radius
        self.available_actions = list(range(board_len  board_len))

    def clear_board(self)
        self.current_player = 1
        self.state = {}
        self.available_actions = list(range(self.board_len  self.board_len))

    def get_available_actions(self) - List[int]
        if len(self.state) == 0
            return list(range(self.board_len  self.board_len))

        positions = np.array(list(self.state.keys()))
        x_coords = positions  self.board_len
        y_coords = positions % self.board_len

        min_x, max_x = x_coords.min(), x_coords.max()
        min_y, max_y = y_coords.min(), y_coords.max()

        min_x = max(min_x - self.search_radius, 0)
        max_x = min(max_x + self.search_radius, self.board_len - 1)
        min_y = max(min_y - self.search_radius, 0)
        max_y = min(max_y + self.search_radius, self.board_len - 1)

        available = []
        for x in range(min_x, max_x + 1)
            for y in range(min_y, max_y + 1)
                action = x  self.board_len + y
                if action not in self.state
                    available.append(action)
        return available

    def do_action(self, action int) - None
        if action not in self.get_available_actions()
            raise ValueError(fAction {action} is not available)
        self.state[action] = self.current_player
        self.current_player = -1

    def is_game_over(self) - Tuple[bool, Union[int, None]]
        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]
        for action, player in self.state.items()
            x, y = divmod(action, self.board_len)
            for dx, dy in directions
                count = 1
                # 向一个方向搜索
                for step in range(1, 5)
                    nx, ny = x + dx  step, y + dy  step
                    if 0 = nx  self.board_len and 0 = ny  self.board_len
                        next_action = nx  self.board_len + ny
                        if self.state.get(next_action, 0) == player
                            count += 1
                        else
                            break
                    else
                        break
                # 向相反方向搜索
                for step in range(1, 5)
                    nx, ny = x - dx  step, y - dy  step
                    if 0 = nx  self.board_len and 0 = ny  self.board_len
                        next_action = nx  self.board_len + ny
                        if self.state.get(next_action, 0) == player
                            count += 1
                        else
                            break
                    else
                        break
                if count = 5
                    return True, player
        if not self.get_available_actions()
            return True, None  # 平局
        return False, None

    def get_feature_planes(self) - torch.Tensor
        self.available_actions = self.get_available_actions()
        planes = np.zeros((self.n_feature_planes, self.board_len, self.board_len), dtype=np.float32)
        actions = np.array(list(self.state.keys()))
        if len(actions)  0
            x = (actions  self.board_len).astype(int)
            y = (actions % self.board_len).astype(int)
            player_values = np.array([1 if player == self.current_player else 0 for player in self.state.values()])
            planes[0][x, y] = player_values

            opponent_values = np.array([1 if player == -self.current_player else 0 for player in self.state.values()])
            planes[1][x, y] = opponent_values

        available = np.array(self.available_actions)
        if len(available)  0
            x_available = (available  self.board_len).astype(int)
            y_available = (available % self.board_len).astype(int)
            planes[2][x_available, y_available] = 1

        return torch.tensor(planes, dtype=torch.float32)


# ================================
# 神经网络模型
# ================================

class ResidualBlock(nn.Module)
    def __init__(self, in_channels=128, out_channels=128)
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.batch_norm1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.batch_norm2 = nn.BatchNorm2d(out_channels)

    def forward(self, x)
        residual = x
        out = self.conv1(x)
        out = self.batch_norm1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.batch_norm2(out)
        out += residual
        out = self.relu(out)
        return out


class PolicyHead(nn.Module)
    def __init__(self, in_channels=128, board_len=15)
        super(PolicyHead, self).__init__()
        self.conv = nn.Conv2d(in_channels, 2, kernel_size=1)
        self.batch_norm = nn.BatchNorm2d(2)
        self.relu = nn.ReLU(inplace=True)
        self.fc = nn.Linear(2  board_len  board_len, board_len  board_len)

    def forward(self, x)
        x = self.conv(x)
        x = self.batch_norm(x)
        x = self.relu(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return F.log_softmax(x, dim=1)


class ValueHead(nn.Module)
    def __init__(self, in_channels=128, board_len=15)
        super(ValueHead, self).__init__()
        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)
        self.batch_norm = nn.BatchNorm2d(1)
        self.relu = nn.ReLU(inplace=True)
        self.fc = nn.Sequential(
            nn.Linear(board_len  board_len, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, 1),
            nn.Tanh()
        )

    def forward(self, x)
        x = self.conv(x)
        x = self.batch_norm(x)
        x = self.relu(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x


class PolicyValueNet(nn.Module)
    def __init__(self, board_len=15, n_feature_planes=17)
        super(PolicyValueNet, self).__init__()
        self.board_len = board_len
        self.conv = nn.Conv2d(n_feature_planes, 128, kernel_size=3, padding=1)
        self.residual_blocks = nn.Sequential(
            ResidualBlock(),
            ResidualBlock(),
            ResidualBlock(),
            ResidualBlock()
        )
        self.policy_head = PolicyHead(in_channels=128, board_len=board_len)
        self.value_head = ValueHead(in_channels=128, board_len=board_len)

    def forward(self, x)
        x = self.conv(x)
        x = self.residual_blocks(x)
        policy = self.policy_head(x)
        value = self.value_head(x)
        return policy, value

    def policy_value(self, x)
        self.eval()
        with torch.no_grad()
            log_p, v = self.forward(x)
            p = torch.exp(log_p)
            return p.numpy()[0], v.numpy()[0][0]

# ================================
# 蒙特卡洛树搜索类
# ================================

class MCTSNode
    def __init__(self, parent, action, prior_p)
        self.parent = parent  # Parent node
        self.children = {}  # Action to MCTSNode
        self.action = action  # The action that led to this node
        self.prior_p = prior_p  # Prior probability from policy network
        self.visit_count = 0
        self.value_sum = 0.0

    def is_leaf(self)
        return len(self.children) == 0

    def Q(self)
        if self.visit_count == 0
            return 0
        return self.value_sum  self.visit_count

    def U(self, c_puct, total_visit)
        return self.prior_p  math.sqrt(total_visit)  (1 + self.visit_count)

    def expand(self, actions, probs)
        for action, prob in zip(actions, probs)
            if action not in self.children
                self.children[action] = MCTSNode(self, action, prob)

    def update(self, value)
        self.visit_count += 1
        self.value_sum += value

    def best_child(self, c_puct)
        best_score = -float('inf')
        best_action = -1
        best_node = None
        total_visit = sum(child.visit_count for child in self.children.values())
        for action, child in self.children.items()
            score = child.Q() + c_puct  child.U(c_puct, total_visit)
            if score  best_score
                best_score = score
                best_action = action
                best_node = child
        return best_action, best_node


class AlphaZeroMCTS
    def __init__(self, policy_value_net PolicyValueNet, c_puct float = 1.4, n_iters int = 50, is_self_play bool = False)
        self.policy_value_net = policy_value_net
        self.c_puct = c_puct
        self.n_iters = n_iters
        self.is_self_play = is_self_play
        self.root = None

    def search(self, chess_board ChessBoard)
        self.root = MCTSNode(parent=None, action=None, prior_p=1.0)
        for _ in range(self.n_iters)
            node = self.root
            board_copy = copy.deepcopy(chess_board)

            # Selection
            while not node.is_leaf()
                action, node = node.best_child(self.c_puct)
                board_copy.do_action(action)

            # Expansion and Evaluation
            game_over, winner = board_copy.is_game_over()
            if not game_over
                feature = board_copy.get_feature_planes().unsqueeze(0)
                policy, value = self.policy_value_net.policy_value(feature)
                available_actions = board_copy.get_available_actions()
                probs = policy[available_actions]
                # Normalize
                probs = np.sum(probs)
                node.expand(available_actions, probs)
            else
                if winner is None
                    value = 0.0
                else
                    value = 1.0 if winner == chess_board.current_player else -1.0
                # Backpropagation
                while node is not None
                    node.update(value)
                    value = -value
                    node = node.parent
                continue

            # Simulation
            if not node.is_leaf()
                # After expansion, we can choose to simulate a random move
                action = random.choice(list(node.children.keys()))
                board_copy.do_action(action)

            game_over, winner = board_copy.is_game_over()
            if game_over
                if winner is None
                    value = 0.0
                else
                    value = 1.0 if winner == chess_board.current_player else -1.0
            else
                feature = board_copy.get_feature_planes().unsqueeze(0)
                _, value = self.policy_value_net.policy_value(feature)
            # Backpropagation
            while node is not None
                node.update(value)
                value = -value
                node = node.parent

    def get_action(self, chess_board ChessBoard, temperature float = 1.0) - Tuple[int, np.ndarray]
        self.search(chess_board)
        counts = np.array([child.visit_count for child in self.root.children.values()])
        actions = list(self.root.children.keys())
        if temperature == 0
            action = actions[np.argmax(counts)]
            pi = np.zeros(chess_board.board_len  chess_board.board_len)
            pi[action] = 1
            return action, pi
        else
            counts = counts  (1  temperature)
            probs = counts  counts.sum()
            action = np.random.choice(actions, p=probs)
            pi = np.zeros(chess_board.board_len  chess_board.board_len)
            pi[action] = 1
            return action, pi

# ================================
# 图形用户界面类
# ================================

class GoBangGUI
    def __init__(self, root, chess_board ChessBoard, mcts 'AlphaZeroMCTS')
        self.root = root
        self.chess_board = chess_board
        self.mcts = mcts
        self.board_len = self.chess_board.board_len
        self.buttons = []
        self.player_move = None  # 用于存储玩家的落子位置

        self.frame = tk.Frame(self.root)
        self.frame.pack()

        # 创建棋盘按钮
        for x in range(self.board_len)
            row_buttons = []
            for y in range(self.board_len)
                btn = tk.Button(self.frame, text='.', width=2, height=1,
                                command=lambda row=x, col=y self.on_player_click(row, col))
                btn.grid(row=x, column=y)
                row_buttons.append(btn)
            self.buttons.append(row_buttons)

        # 游戏状态标签
        self.status_label = tk.Label(self.root, text=Player(B) turn)
        self.status_label.pack()

        # 更新棋盘
        self.update_board_display()

    def on_player_click(self, x, y)
        action = x  self.board_len + y
        if action in self.chess_board.get_available_actions()
            self.player_move = action
            self.root.after(100, self.player_move_and_ai_turn)  # 确保UI更新不被阻塞
        else
            messagebox.showinfo(Invalid Move, This position is not available!)

    def update_board_display(self)
        board = np.zeros((self.board_len, self.board_len), dtype=int)
        for action, player in self.chess_board.state.items()
            row, col = divmod(action, self.board_len)
            board[row, col] = player

        for x in range(self.board_len)
            for y in range(self.board_len)
                val = board[x, y]
                if val == 1
                    self.buttons[x][y].config(text='B', bg='black', fg='white')
                elif val == -1
                    self.buttons[x][y].config(text='W', bg='white', fg='black')
                else
                    self.buttons[x][y].config(text='.', bg='SystemButtonFace')

    def player_move_and_ai_turn(self)
        if self.player_move
            try
                self.chess_board.do_action(self.player_move)
                self.update_board_display()

                game_over, winner = self.chess_board.is_game_over()
                if game_over
                    self.show_game_result(winner)
                    return

                # AI 思考并落子
                self.status_label.config(text=Computer(W) is thinking...)
                self.root.update()  # 更新状态标签

                action, _ = self.mcts.get_action(self.chess_board, temperature=0)
                self.chess_board.do_action(action)
                self.update_board_display()

                game_over, winner = self.chess_board.is_game_over()
                if game_over
                    self.show_game_result(winner)
                else
                    self.status_label.config(text=Player(B) turn)

                self.player_move = None

            except ValueError
                messagebox.showinfo(Error, Invalid Move.)
                return

    def show_game_result(self, winner)
        if winner is None
            message = It's a draw!
        elif winner == 1
            message = Player(B) wins!
        else
            message = Computer(W) wins!
        messagebox.showinfo(Game Over, message)
        self.root.quit()  # 游戏结束，退出程序

# ================================
# 主程序入口
# ================================

def main()
    board_len = 15
    n_feature_planes = 17

    # 初始化策略价值网络
    policy_value_net = PolicyValueNet(board_len=board_len, n_feature_planes=n_feature_planes)
    # 假设模型已经训练好并加载权重
    # policy_value_net.load_state_dict(torch.load('policy_value_net.pth'))
    policy_value_net.eval()  # 设置为评估模式

    # 初始化棋盘
    chess_board = ChessBoard(board_len=board_len, n_feature_planes=n_feature_planes)

    # 初始化 AlphaZeroMCTS
    mcts = AlphaZeroMCTS(policy_value_net=policy_value_net, c_puct=1.4, n_iters=1000, is_self_play=False)

    # 初始化 Tkinter 界面
    root = tk.Tk()
    root.title(五子棋AI)

    # 初始化 GoBangGUI
    gui = GoBangGUI(root, chess_board, mcts)
    root.mainloop()

if __name__ == __main__
    main()
